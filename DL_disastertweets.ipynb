{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_disastertweets.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUykf06RTLiA",
        "outputId": "06acf66a-f2f6-4954-ad7e-85e3d68a3a70"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu==1.15\n",
            "  Downloading tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5 MB 7.2 kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 62.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.17.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.19.5)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 42.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.39.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.37.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (4.6.4)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15) (1.5.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.5.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=a77a4b125ecdd0acfc6afda219e4aa5474f0110afef532b5bb597f6d9b1c5308\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow-gpu\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.6.0\n",
            "    Uninstalling tensorboard-2.6.0:\n",
            "      Successfully uninstalled tensorboard-2.6.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires gast==0.4.0, but you have gast 0.2.2 which is incompatible.\n",
            "tensorflow 2.6.0 requires tensorboard~=2.6, but you have tensorboard 1.15.0 which is incompatible.\n",
            "tensorflow 2.6.0 requires tensorflow-estimator~=2.6, but you have tensorflow-estimator 1.15.1 which is incompatible.\n",
            "tensorflow-probability 0.13.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHszJg-ETR2v",
        "outputId": "2a65e4ac-bada-4875-fb9c-92e44e1621a7"
      },
      "source": [
        "!pip install -U bert-serving-server bert-serving-client"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert-serving-server\n",
            "  Downloading bert_serving_server-1.10.0-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 40 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 51 kB 3.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 61 kB 3.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 202 kB/s \n",
            "\u001b[?25hCollecting bert-serving-client\n",
            "  Downloading bert_serving_client-1.10.0-py2.py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: pyzmq>=17.1.0 in /usr/local/lib/python3.7/dist-packages (from bert-serving-server) (22.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bert-serving-server) (1.19.5)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from bert-serving-server) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-serving-server) (1.15.0)\n",
            "Collecting GPUtil>=1.3.0\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=aadd10743d9d3d7eedf471bc56b5c1fdff00c5fa5d0de09732c72b75152f2704\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil, bert-serving-server, bert-serving-client\n",
            "Successfully installed GPUtil-1.4.0 bert-serving-client-1.10.0 bert-serving-server-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zDvLFXxTlfP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Hjy5x22ECYB"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding, Dropout\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndm94g-REEbs"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wjT_1zeEFPj"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/Sharing Caring/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Sharing Caring/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeoGkQb7Fe0S"
      },
      "source": [
        "train.text=train.text.apply(lambda x:x.lower() )\n",
        "test.text=test.text.apply(lambda x:x.lower())\n",
        "#removing square brackets\n",
        "train.text=train.text.apply(lambda x:re.sub('\\[.*?\\]', '', x) )\n",
        "test.text=test.text.apply(lambda x:re.sub('\\[.*?\\]', '', x) )\n",
        "train.text=train.text.apply(lambda x:re.sub('<.*?>+', '', x) )\n",
        "test.text=test.text.apply(lambda x:re.sub('<.*?>+', '', x) )\n",
        "#removing hyperlink\n",
        "train.text=train.text.apply(lambda x:re.sub('https?://\\S+|www\\.\\S+', '', x) )\n",
        "test.text=test.text.apply(lambda x:re.sub('https?://\\S+|www\\.\\S+', '', x) )\n",
        "#removing puncuation\n",
        "train.text=train.text.apply(lambda x:re.sub('[%s]' % re.escape(string.punctuation), '', x) )\n",
        "test.text=test.text.apply(lambda x:re.sub('[%s]' % re.escape(string.punctuation), '', x) )\n",
        "train.text=train.text.apply(lambda x:re.sub('\\n' , '', x) )\n",
        "test.text=test.text.apply(lambda x:re.sub('\\n', '', x) )\n",
        "#remove words containing numbers\n",
        "train.text=train.text.apply(lambda x:re.sub('\\w*\\d\\w*' , '', x) )\n",
        "test.text=test.text.apply(lambda x:re.sub('\\w*\\d\\w*', '', x) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqX2gNrjFgpO"
      },
      "source": [
        "token = Tokenizer()\n",
        "token.fit_on_texts(train['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cy4HTqPDFiZ-",
        "outputId": "4c6d7f35-e729-4520-a5f4-aabc2ead8650"
      },
      "source": [
        "vocab_size = len(token.word_index) + 1\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16834"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5Y_4JXOVfHi"
      },
      "source": [
        "encode = token.texts_to_sequences(train['text'])\n",
        "x = pad_sequences(encode, maxlen = 40, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8xBn5U8FkGU",
        "outputId": "25502f73-ea6b-49fd-8ed5-48e3cecc8ee0"
      },
      "source": [
        "# encode = token.texts_to_sequences(train['text'])\n",
        "# x = pad_sequences(encode, maxlen = 40, padding='post')\n",
        "\n",
        "vec_size = 100\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, vocab_size, input_length = 40))\n",
        "\n",
        "model.add(Conv1D(32, 2, activation='relu'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(x, train['target'], epochs = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 40, 16834)         283383556 \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 39, 32)            1077408   \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 19, 32)            0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 19, 32)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 19, 32)            1056      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 19, 32)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 19, 16)            528       \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 284,462,565\n",
            "Trainable params: 284,462,565\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 7613 samples\n",
            "Epoch 1/10\n",
            "7613/7613 [==============================] - 839s 110ms/sample - loss: 0.5691 - acc: 0.7009\n",
            "Epoch 2/10\n",
            "7613/7613 [==============================] - 842s 111ms/sample - loss: 0.4158 - acc: 0.8208\n",
            "Epoch 3/10\n",
            "7613/7613 [==============================] - 842s 111ms/sample - loss: 0.3106 - acc: 0.8756\n",
            "Epoch 4/10\n",
            "7613/7613 [==============================] - 837s 110ms/sample - loss: 0.2240 - acc: 0.9183\n",
            "Epoch 5/10\n",
            "7613/7613 [==============================] - 857s 113ms/sample - loss: 0.1608 - acc: 0.9452\n",
            "Epoch 6/10\n",
            "7613/7613 [==============================] - 855s 112ms/sample - loss: 0.1292 - acc: 0.9576\n",
            "Epoch 7/10\n",
            "7613/7613 [==============================] - 850s 112ms/sample - loss: 0.1119 - acc: 0.9628\n",
            "Epoch 8/10\n",
            "7613/7613 [==============================] - 849s 111ms/sample - loss: 0.0980 - acc: 0.9670\n",
            "Epoch 9/10\n",
            "7613/7613 [==============================] - 865s 114ms/sample - loss: 0.0847 - acc: 0.9727\n",
            "Epoch 10/10\n",
            "7613/7613 [==============================] - 876s 115ms/sample - loss: 0.0773 - acc: 0.9739\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f08abbfec50>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRn3QAErGPof"
      },
      "source": [
        "encode_test = token.texts_to_sequences(test['text'])\n",
        "x_test = pad_sequences(encode_test, maxlen = 40, padding='post')\n",
        "pred = model.predict_classes(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x18HOFT8XNmD",
        "outputId": "7fdd1ba5-1c27-4ce0-80bc-dd08c8595a97"
      },
      "source": [
        "pred.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3263, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-QRXmyLodIk",
        "outputId": "5c578608-4294-4f36-ab80-a6309c7b858f"
      },
      "source": [
        "pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [1],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [1],\n",
              "       [1]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wh4wcHXpC_D"
      },
      "source": [
        "test['target'] = pred\n",
        "test = test.drop(columns=['keyword', 'location', 'text'])\n",
        "test.to_csv(r'test_dl.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q5kUrf1psGT"
      },
      "source": [
        "test.to_csv(r'/content/drive/MyDrive/Sharing Caring/test_dl_1.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC5D21PMp9sC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}